{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction de prix de vente de maisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyppeteer\n",
      "  Using cached pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\hp\\anaconda3\\envs\\pyfinance\\lib\\site-packages (from pyppeteer) (4.11.3)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\hp\\anaconda3\\envs\\pyfinance\\lib\\site-packages (from pyppeteer) (1.26.12)\n",
      "Collecting pyee<9.0.0,>=8.1.0\n",
      "  Using cached pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tqdm<5.0.0,>=4.42.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2021 in c:\\users\\hp\\anaconda3\\envs\\pyfinance\\lib\\site-packages (from pyppeteer) (2022.9.14)\n",
      "Collecting appdirs<2.0.0,>=1.4.3\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting websockets<11.0,>=10.0\n",
      "  Downloading websockets-10.3-cp310-cp310-win_amd64.whl (98 kB)\n",
      "     ---------------------------------------- 98.6/98.6 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\envs\\pyfinance\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer) (3.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\pyfinance\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer) (0.4.5)\n",
      "Installing collected packages: pyee, appdirs, websockets, tqdm, pyppeteer\n",
      "Successfully installed appdirs-1.4.4 pyee-8.2.2 pyppeteer-1.0.2 tqdm-4.64.1 websockets-10.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pyppeteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliotheques \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Contrôle l'affichage des colonnes\n",
    "#https://towardsdatascience.com/8-commonly-used-pandas-display-options-you-should-know-a832365efa95\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AmesHousing.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# importer les données \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAmesHousing.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyfinance\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyfinance\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyfinance\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyfinance\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyfinance\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyfinance\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AmesHousing.tsv'"
     ]
    }
   ],
   "source": [
    "# importer les données \n",
    "data = pd.read_csv(\"AmesHousing.tsv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afficher la forme de la table \n",
    "print(data.shape)\n",
    "print(len(str(data.shape))*'-')\n",
    "\n",
    "#Afficher les totaux de diférents types de colonnes \n",
    "print(data.dtypes.value_counts())\n",
    "\n",
    "#Afficher la table \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions permettant de calculer l'erreur quadratique moyenne de la regression de linéaire \n",
    "# Plus l'erreur est faible plus le modéle est proche de la réalité\n",
    "def transform_features(df):\n",
    "    return df\n",
    "\n",
    "def select_features(df):\n",
    "    return df[[\"Gr Liv Area\", \"SalePrice\"]]\n",
    "\n",
    "def train_and_test(df):\n",
    "    train = df[:1460]\n",
    "    test = df[1460:]\n",
    "    numeric_train = train.select_dtypes(include=['integer', 'float'])\n",
    "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    features = numeric_train.columns.drop(\"SalePrice\")\n",
    "    \n",
    "    # Entrainement\n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(train[features], train[\"SalePrice\"])\n",
    "    \n",
    "    # Prédiction\n",
    "    predictions = lr.predict(test[features])\n",
    "    mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application de \n",
    "transform_df = transform_features(data)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reamrque que sans faire de prétraitement au préalable de nos caractéristiques, de la sélection des caractéristiques, l'erreur quadratique moyenne (rmse) est très grande. Nous allons faire un peu de traitement pour voir s'il y a un changment de valeur de rmse. Il faut noter que le modéle de régression linéaire est sensible au probléme de dimension, appelée communement la malédiction de la dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des caractéristiques\n",
    "\n",
    "Le succés de la modélisation predictive depend fortement des qualités des caracteristiques du modéle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gérer les valeurs manquantes:\n",
    "- Toutes les colonnes:\n",
    " + Supprimer celles qui ont 5% ou plus de valeurs manquantes pour le moment.\n",
    "- Colonnes texte:\n",
    " + Supprimer toute colonne contenant 1 valeur manquante ou plus pour le moment.\n",
    "- Colonnes numériques:\n",
    " + Pour les colonnes contenant des valeurs manquantes, remplacer par les valeurs les plus fréquentes de la colonne\n",
    " \n",
    "**B :** il faut absolument être prudent avec les valeurs manquantes leur totale supression peut causer la perte d'informations qui pourraient être utiles. Donc, on doit trouver des combines pour optimiser leur gestion et ainsi améliorer les performances du modéle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Toutes les colonnes:** supprimer celles qui ont 5% ou plus de valeurs manquantes pour le moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de variables manquantes\n",
    "num_missing = data.isnull().sum()\n",
    "\n",
    "# Filtrer l'objet Series sur les colonnes contenant > 5% de valeurs manquantes\n",
    "drop_missing_cols = num_missing[(num_missing > len(data)/20)].sort_values()\n",
    "\n",
    "# Supprimer ces colonnes dans le DataFrame. Noter l'utilisation de l'accesseur .index\n",
    "data = data.drop(drop_missing_cols.index, axis=1)\n",
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que des 82 colonnes, on est passé à 71 en supprimant toutes les colonnes ayant plus de 5% de valeurs manquantes. On continue le pretraitement de colonnes texte dont on appelle communement des variables catégorielles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Colonnes texte :** supprimer toute colonne contenant au moins une valeur manquante pour le moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mv_counts = data.select_dtypes(include=['object']).isnull().sum().sort_values(\n",
    "    ascending=False)\n",
    "\n",
    "# Filtrer l'objet Series sur les colonnes contenant au moins une valeur manquante\n",
    "drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n",
    "data = data.drop(drop_missing_cols_2.index, axis=1)\n",
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate qu'on a passé de 71 colonnes à 64 en supprimant toutes les variables catégorielles ayent au moins une valeur manquante "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Colonnes numériques :** pour les colonnes contenant des valeurs manquantes, remplacer par la valeur la plus fréquente de la colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le nombre de valeurs manquantes colonne par colonne\n",
    "num_missing = data.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "\n",
    "# récuperer et trier le 5% de valeurs manquantes des colonnes numériques \n",
    "fixable_numeric_missing_cols = num_missing[(num_missing < len(data)/20) & \n",
    "                                   (num_missing > 0)].sort_values()\n",
    "fixable_numeric_missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la valeur la plus commune pour chaque colonne.\n",
    "replacement_values_dict = data[fixable_numeric_missing_cols.index].mode().to_dict(\n",
    "    orient='records')[0]\n",
    "replacement_values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les valeurs manquantes.\n",
    "data = data.fillna(replacement_values_dict)\n",
    "\n",
    "## Vérifier que toutes les colonnes ont bien 0 valeur manquante\n",
    "data.isnull().sum().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons fini avec les valeurs manquantes. Nous allos continuer à faire de feature engineering en créant des nouvelles caractéristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quelles nouvelles caractéristiques pouvons-nous créer pour mieux capturer les informations contenues dans certaines caractéristiques?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les dates de vente moins les dates de construction \n",
    "years_sold = data['Yr Sold'] - data['Year Built']\n",
    "years_sold[years_sold < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons une maison qui a été vendue 1 an avant sa constrcution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparons les dates de ventes et de renovation\n",
    "years_since_remod = data['Yr Sold'] - data['Year Remod/Add']\n",
    "years_since_remod[years_since_remod < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer 2 nouvelles colonnes\n",
    "data['Years Before Sale'] = years_sold\n",
    "data['Years Since Remod'] = years_since_remod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les lignes avec des valeurs négatives pour ces nouvelles caractéristiques\n",
    "data = data.drop([1702, 2180, 2181], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plus besoin des colonnes de l'année d'origine\n",
    "data = data.drop([\"Year Built\", \"Year Remod/Add\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supprimer les colonnes qui:\n",
    "- Ne sont pas utiles pour le Machine Learning\n",
    "- Fuite des données au sujet de la vente finale (cf. documentation dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les colonnes qui ne sont pas utiles pour le ML\n",
    "data = data.drop([\"PID\", \"Order\"], axis=1)\n",
    "\n",
    "# Supprimer les colonnes qui font fuiter des informations sur la vente finale\n",
    "data = data.drop([\"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Yr Sold\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une fonction contenant tous les pretraitements des caractéristiques rélisés ci-haut\n",
    "\n",
    "def transform_features(df, percent_missing=0.05):\n",
    "    num_missing = df.isnull().sum()\n",
    "    drop_missing_cols = num_missing[(num_missing > len(df)*percent_missing)].sort_values()\n",
    "    df = df.drop(drop_missing_cols.index, axis=1)\n",
    "    \n",
    "    text_mv_counts = df.select_dtypes(include=['object']).isnull().sum().sort_values(\n",
    "        ascending=False)\n",
    "    drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n",
    "    df = df.drop(drop_missing_cols_2.index, axis=1)\n",
    "    \n",
    "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "    fixable_numeric_cols = num_missing[(num_missing < len(df)*percent_missing) & \n",
    "                                       (num_missing > 0)].sort_values()\n",
    "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(\n",
    "        orient='records')[0]\n",
    "    df = df.fillna(replacement_values_dict)\n",
    "    \n",
    "    years_sold = df['Yr Sold'] - df['Year Built']\n",
    "    years_since_remod = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    df['Years Before Sale'] = years_sold\n",
    "    df['Years Since Remod'] = years_since_remod\n",
    "    df = df.drop([1702, 2180, 2181], axis=0)\n",
    "    df = df.drop([\"PID\", \"Order\", \"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Year Built\", \n",
    "                  \"Year Remod/Add\"], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Fonction retournant deux caractéristiques\n",
    "def select_features(df):\n",
    "    return df[[\"Gr Liv Area\", \"SalePrice\"]]\n",
    "\n",
    "#Fonction rettournant la valeur de rmse \n",
    "def train_and_test(df):\n",
    "    train = df[:1460]\n",
    "    test = df[1460:]\n",
    "    numeric_train = train.select_dtypes(include=['integer', 'float'])\n",
    "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    features = numeric_train.columns.drop(\"SalePrice\")\n",
    "    \n",
    "    # Entrainement\n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(train[features], train[\"SalePrice\"])\n",
    "    \n",
    "    # Prédiction\n",
    "    predictions = lr.predict(test[features])\n",
    "    mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"AmesHousing.tsv\", delimiter=\"\\t\")\n",
    "transform_df = transform_features(data)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir traité et retravaillé nos caractéristiques, on obtient une autre valeur d'erreur de 55275.367. Donc, on peut dire on a amélioré notre modéle simplement en transformant nos caractéristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection des caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectinner les colonnes numériques\n",
    "numerical_df = transform_df.select_dtypes(include=['int', 'float'])\n",
    "numerical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimer la correlation de la colonne cible (SalePrice) avec les différentes caracteristiques \n",
    "abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
    "abs_corr_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne gardons que les colonnes avec un coefficient de corrélation supérieur à 0.4 \n",
    "# (arbitraire, à tester plus tard!)\n",
    "abs_corr_coeffs[abs_corr_coeffs > 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les colonnes avec une corrélation inférieure à 0.4 avec SalePrice\n",
    "transform_df = transform_df.drop(abs_corr_coeffs[abs_corr_coeffs < 0.4].index, axis=1)\n",
    "transform_df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles colonnes catégoriques devrions-nous garder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une liste de noms de colonne à partir de la documentation qui sont censés être \n",
    "# catégoriques\n",
    "nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\",\n",
    "                    \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \n",
    "                    \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \"Exterior 2nd\", \n",
    "                    \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\",\n",
    "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quelles colonnes sont actuellement numériques mais doivent plutôt être codées en tant que catégoriques (car les nombres n'ont aucun signification sémantique)?\n",
    "- Si une colonne catégorique contient des centaines de valeurs uniques (ou catégories), devrions-nous la conserver? Lorsque nous rendons cette colonne factice, des centaines de colonnes (pour chacune des catégories) devront être rajoutées au DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_cat_cols = []\n",
    "for col in nominal_features:\n",
    "    if col in transform_df.columns:\n",
    "        transform_cat_cols.append(col)\n",
    "transform_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combien de valeurs uniques dans chaque colonne catégorique?\n",
    "uniqueness_counts = transform_df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "\n",
    "# Limite arbitraire de 10 valeurs uniques (expérimentation)\n",
    "drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
    "transform_df = transform_df.drop(drop_nonuniq_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner uniquement les colonnes de texte restantes et convertissez-les en catégories\n",
    "text_cols = transform_df.select_dtypes(include=['object'])\n",
    "for col in text_cols:\n",
    "    transform_df[col] = transform_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer des colonnes factices et ajouter les au DataFrame\n",
    "transform_df = pd.concat([\n",
    "    transform_df,\n",
    "    pd.get_dummies(transform_df.select_dtypes(include=['category']))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction realisant les étapes de la sélection des caractéristiuqes \n",
    "def select_features(df, coeff_threshold=0.4, uniq_threshold=10):\n",
    "    numerical_df = df.select_dtypes(include=['int', 'float'])\n",
    "    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
    "    df = df.drop(abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)\n",
    "    \n",
    "    nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\",\n",
    "                        \"Lot Config\", \"Neighborhood\", \"Condition 1\", \"Condition 2\", \n",
    "                        \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\",\"Exterior 1st\",\n",
    "                        \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \n",
    "                        \"Central Air\", \"Garage Type\",\"Misc Feature\", \"Sale Type\", \n",
    "                        \"Sale Condition\"]\n",
    "    \n",
    "    transform_cat_cols = []\n",
    "    for col in nominal_features:\n",
    "        if col in df.columns:\n",
    "            transform_cat_cols.append(col)\n",
    "            \n",
    "    uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "    drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > uniq_threshold].index\n",
    "    df = df.drop(drop_nonuniq_cols, axis=1)\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=['object'])\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "    df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=['category']))], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(df, k=0):\n",
    "    numeric_df = df.select_dtypes(include=['integer', 'float'])\n",
    "    features = numeric_df.columns.drop(\"SalePrice\")\n",
    "    lr = linear_model.LinearRegression()\n",
    "    \n",
    "    if k == 0:\n",
    "        train = df[:1460]\n",
    "        test = df[1460:]\n",
    "        \n",
    "        lr.fit(train[features], train[\"SalePrice\"])\n",
    "        predictions = lr.predict(test[features])\n",
    "        \n",
    "        mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        return rmse\n",
    "    \n",
    "    if k == 1:\n",
    "        shuffled_df = df.sample(frac=1, )\n",
    "        train = df[:1460]\n",
    "        test = df[1460:]\n",
    "        \n",
    "        lr.fit(train[features], train[\"SalePrice\"])\n",
    "        predictions_one = lr.predict(test[features])\n",
    "        \n",
    "        mse_one = mean_squared_error(test[\"SalePrice\"], predictions_one)\n",
    "        rmse_one = np.sqrt(mse_one)\n",
    "        \n",
    "        lr.fit(test[features], test[\"SalePrice\"])\n",
    "        predictions_two = lr.predict(train[features])\n",
    "        \n",
    "        mse_two = mean_squared_error(train[\"SalePrice\"], predictions_two)\n",
    "        rmse_two = np.sqrt(mse_two)\n",
    "        \n",
    "        avg_rmse = np.mean([rmse_one, rmse_two])\n",
    "        print(rmse_one)\n",
    "        print(rmse_two)\n",
    "        \n",
    "        return avg_rmse\n",
    "    \n",
    "    else:\n",
    "        kf = KFold(n_splits=k, shuffle=True)\n",
    "        rmse_values = []\n",
    "        \n",
    "        for train_index, test_index, in kf.split(df):\n",
    "            train = df.iloc[train_index]\n",
    "            test = df.iloc[test_index]\n",
    "            \n",
    "            lr.fit(train[features], train[\"SalePrice\"])\n",
    "            predictions = lr.predict(test[features])\n",
    "            \n",
    "            mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            rmse_values.append(rmse)\n",
    "        \n",
    "        print(rmse_values)\n",
    "        avg_rmse = np.mean(rmse_values)\n",
    "        \n",
    "        return avg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"AmesHousing.tsv\", delimiter='\\t')\n",
    "\n",
    "transformed_data = transform_features(data)\n",
    "final_data = select_features(transformed_data)\n",
    "\n",
    "results = []\n",
    "for i in range(100):\n",
    "    result = train_and_test(final_data, k=i)\n",
    "    results.append(result)\n",
    "    \n",
    "x = [i for i in range(100)]\n",
    "y = results \n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Kfolds')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au final l'erreur le plus faible est atteinte pour le K le plus grand. Bref, la validation croisée nous a permis d'évaluer la performance du mondéle on quasiment la rmse en deux avec k avoisiant le 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
